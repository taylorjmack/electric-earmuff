# electric-earmuff

## Sentiment Analysis on Yelp user Reviews
Do Yelp reviews contain specific words that determine whether a user will ultimately give a positive or negative review?

### Motivation
As a society we have put a major focus on reviewing products, services, movies, restaurants, and various other aspects of our daily lives. Since this is the case, positive and negative reviews carry weight on whether a consumer will ultimately buy a product or service. Being negatively reviewed can have major impacts on businesses, especially small businesses, to the point where consumers will no longer even consider purchasing a product/service unless it is at a certain positive review threshold. I personally believe that there are users that take advantage of the power they have as a reviewer and will either mistakenly or maliciously submit a review rating that doesn’t match the wording of their review. This topic opens what my machine learning project is. I wanted to investigate if it is possible to conduct a sediment analysis on the wording that is used on a review and determine whether the review will be either positive or negative.

### The Dataset
The dataset that I used is generated by Yelp and can be found at https://www.yelp.com/dataset. This dataset is exported as 6 separate JSON (JavaScript Object Notation) files which are business.json, review.json, user.json, checkin.json, tip.json, photo.json) for my analysis I only utilized the review.json file. This file contains 6,990,280 reviews from 1,987,897 users on 150,346 different businesses. The reviews are only in English, and the contains the following features:

| Field Name | Data Type |          Short Description           |
|:----------:|:------:|:------------------------------------:|
| review_id  |   String   |           Unique review ID           |
|  user_id  |    String   |            Unique user id            |
|  business_id  |    String   |          Unique business ID          |
|  stars  |      Integer     |     Review rating on a 1-5 scale     |
|  date  |    String   |    Date of review in ‘YYYY-MM-DD’    |
|  text  |    String   |          Review text itself          |
|  userful  |      Integer   |   Number of useful votes received    |
|  funny  |      Integer   |    Number of funny votes received    |
|  cool  |      Integer   |    Number of cool votes received     |
|  Result  |      Integer   | 1 if ‘stars’ is >4, 0 if‘stars’is <3 |

Field Name Data Type
review_id     String user_id     String business_id     String stars     Integer date     String text     String userful     Integer funny     Integer cool     Integer Result     Integer

### Model
The two models I discuss later in the report and use the ‘Result’ feature as the dependent variable.

### Sentiment Analysis

This concept is the approach of analyzing natural language processing (NLP) to get intent, tone, or emotion from a body of text. There are four major types of sentiment analysis, but for the purposes of this report we will only be focusing in on fine-grained sentiment analysis. Fine-grained sentiment analysis breaks down a body of text into either positive or negative classifiers, which work well with opinion ratings such as that found on the Yelp dataset. In my analysis I used two different models: 1) Keras One-Hot with Neural Networks 2) Sklearn CountVectorizer with LogisticRegression

### Keras One-Hot encoding with Neural Networks (Model 1)

The biggest issue with Sentiment analysis is that our machine learning models do not understand words, but they do understand numbers which is where Keras One-Hot encoding comes into play. One-Hot encoding takes every individual word on a review and converts the word into an integer. After that process has been completed One-Hot encoding will then convert that integer into a binary vector, which will then be fed into our model as the independent variable.

### Sklearn CountVectorizer with Logistical Regression (Model 2)

Sklearn CountVectorizer create a vector of token that are generated by how often a words occurred in the body of text in a review. After those vectors have been created, I utilize a logistical regression since our dependent variable is a binary variable.

### Data Wrangling

In order to load the JSON dataset into python I utilized the pandas library to convert the dataset into a DataFrame, via the read_json function. Since this the dataset is quite large, with 6.9 million responses the file size ended up being approximately 5.2 gigabytes of data, I decided to parse down the data to only include reviews from 2021. I then created binary classification field called ‘Result’ column which assigned a value of “1” if the ‘stars’ rating was higher than 3, and a value of “0” if the ‘stars’ rating was less than 3. Reviews that had a ‘stars’ rating of 3 were removed from the dataset because they were determined to be neutral and could potentially skew both models.

### Keras - One Hot Model (Model One)

Due to computer processing power constraints I decided to further limit the dataset down to the top 100,000 rows. One major benefit of the Keras and TensorFlow library is that the model processing can concurrently utilize CPU and GPU resources, instead of just the CPU’s resources. For the One-Hot encoding function, after testing a few different options I thought it was best to limit the vocabulary size to be 10,000 of the most popular words and all other words would be ignored for this model. Another filter that I included is to limit every review to just be a maximum of 120 words long. On the model I included an embedding layer that also had a vocabulary of 10,000 words and 128 neurons. Since this is a binary classification model a Sigmoid activation function was utilized. The training and test datasets were 80% and 20% respectively.

### Sklearn – CountVectorizer (Model Two)

Due to the same computing constraints as in Model 1 forced me to limit down the dataset to 20,000 lines. After converting the words into vectors, I then put those vectors into a logistical regression with a maximum iteration of 1000 to ensure that the regression would reach convergence. Similarly to model 1 this model used an 80/20 split between training and test datasets.

### Results

Model 1 Accuracy: .9439 

Confusion Matrix

|          | Positive | Negative |
|:--------:|:--------:|:--------:|
| Positive |   13474    |   676    |
| Negative |    411     |    5439  |

Model 2 Accuracy: .95325 

Confusion Matrix

|          | Positive | Negative |
|:--------:|:--------:|:--------:|
| Positive |   951    |   107    |
| Negative |    80     |    2862  |

Both models exhibit very high accuracy and had shockingly similar accuracy given that they approach sentiment analysis in completely different ways. They both show that they can effectively classify reviews as positive or negative based on the wording of the reviews.

### Model Choice

Ultimately it is difficult to directly compare the two models without having to force model 1 to operate off a smaller dataset due to the computing inefficiencies caused by model 2’s algorithm. The preferred model should be Model 1, even though it does have slightly lower accuracy. I believe that if Model 2 had been given the opportunity to run on a larger dataset it would have experienced diminishing accuracy, due to the simplistic nature of the model. Model 1’s algorithm ran more efficiently on limited computing capabilities and is more modular with its ability of reducing certain aspects of the model without having to necessarily reduce the dataset size.

### Conclusion

Both models displayed that they were effective in predicting whether a review was going to be either positive or negative depending on the words that were written on the review. Our next step is to conduct an analysis on the reviews that flowed into the False Positive or False Negative. We also need to share this research with others to verify its validity and determine if all reviews that are classified as either FalsePositive or FalseNegative in this model should be removed for a business’s overall Yelp rating.

### References

Contributor, T. (2021, October 15). What is sentiment analysis (opinion mining)? definition from Searchbusinessanalytics. Retrieved December 4, 2022, from https://www.techtarget.com/searchbusinessanalytics/definition/opinion-mining-sentiment- mining
Brownlee, J. (2020, May 27). How to use the keras functional API for Deep Learning. Retrieved December 4, 2022, from https://machinelearningmastery.com/keras-functional-api-deep- learning/#:~:text=Keras%20Sequential%20Models,- As%20a%20review&text=The%20Sequential%20model%20API%20is,created%20and%20added%2 0to%20it.
Novack, G. (2020, June 08). Building a one hot encoding layer with tensorflow. Retrieved December 4, 2022, from https://towardsdatascience.com/building-a-one-hot-encoding-layer-with-tensorflow- f907d686bf39
Using countvectorizer to extracting features from text. (2022, July 07). Retrieved December 4, 2022, from https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/











